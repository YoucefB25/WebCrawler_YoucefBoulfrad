{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Youcef Boulfrad\n",
    "\n",
    "Master STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in /home/ensai/.local/lib/python3.10/site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/ensai/.local/lib/python3.10/site-packages (4.12.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ensai/.local/lib/python3.10/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests) (1.26.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ensai/.local/lib/python3.10/site-packages (from beautifulsoup4) (2.5)\n",
      "Configuration initiale termin√©e !\n"
     ]
    }
   ],
   "source": [
    "# Partie 1 : Configuration initiale \n",
    "\n",
    "## Installation des biblioth√®ques requises\n",
    "!pip install requests beautifulsoup4\n",
    "\n",
    "## Cr√©ation de la structure du projet\n",
    "\n",
    "# En bash : \n",
    "# mkdir -p Web_Crawler\n",
    "# cd Web_Crawler\n",
    "\n",
    "# Ouverture du dossier avec VSCODE\n",
    "\n",
    "# Cr√©ation d'un Repo GitHub distant, et local, et utilisation des commandes GIT pour le mettre √† jour\n",
    "\n",
    "## Importation des modules n√©cessaires\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time\n",
    "import urllib.robotparser\n",
    "import json\n",
    "import re\n",
    "\n",
    "## Impl√©mentation des fonctions de base pour les requ√™tes HTTP\n",
    "def telecharger_page(url):\n",
    "    \"\"\"T√©l√©charge le contenu HTML d'une URL donn√©e.\"\"\"\n",
    "    try:\n",
    "        en_tetes = {\"User-Agent\": \"Explorateur-ENSAI\"}\n",
    "        reponse = requests.get(url, headers=en_tetes, timeout=5)\n",
    "        reponse.raise_for_status()\n",
    "        return reponse.text\n",
    "    except requests.RequestException as e: # pour lever une exception en cas d'√©chec du t√©l√©chargement\n",
    "        print(f\" √âchec du t√©l√©chargement {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "## Ajout de la notion de politesse\n",
    "def attendre():\n",
    "    \"\"\"Ajoute un d√©lai pour √©viter de surcharger le serveur (politesse).\"\"\"\n",
    "    time.sleep(1)  # Attente de 1 seconde entre chaque requ√™te\n",
    "\n",
    "print(\"Configuration initiale termin√©e !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction du contenu pr√™te √† √™tre utilis√©e !\n"
     ]
    }
   ],
   "source": [
    "# Partie 2 : Extraction du contenu d'une page HTML\n",
    "\n",
    "## V√©rification des droits de parsing via robots.txt (pour une URL donn√©e)\n",
    "def autorise_crawl(url):\n",
    "    \"\"\"V√©rifie si le crawler a le droit de parser une page selon robots.txt.\"\"\"\n",
    "    robot_parser = urllib.robotparser.RobotFileParser()\n",
    "    domaine = \"{uri.scheme}://{uri.netloc}\".format(uri=urlparse(url))\n",
    "    robots_url = urljoin(domaine, \"/robots.txt\")\n",
    "    robot_parser.set_url(robots_url)\n",
    "    try:\n",
    "        robot_parser.read()\n",
    "        return robot_parser.can_fetch(\"*\", url)\n",
    "    except:                           # l√®ve une exception sur l'URL n'autorise pas l'extraction de contenu\n",
    "        print(f\"Impossible de lire robots.txt sur {robots_url}\")\n",
    "        return False\n",
    "\n",
    "## Fonction pour parser le HTML (c'est √† dire extraire les principales informations d'une page HTML)\n",
    "def parser_html(html, url):\n",
    "    \"\"\"Parse le contenu HTML et extrait titre, premier paragraphe et liens.\"\"\"\n",
    "    soupe = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    # Extraction du titre\n",
    "    titre = soupe.title.string.strip() if soupe.title else \"Sans titre\"\n",
    "    \n",
    "    # Extraction du premier paragraphe\n",
    "    premier_paragraphe = \"\"\n",
    "    paragraphes = soupe.find_all(\"p\")\n",
    "    if paragraphes:\n",
    "        premier_paragraphe = paragraphes[0].get_text().strip()\n",
    "    \n",
    "    # Extraction des liens internes et source\n",
    "    domaine = \"{uri.scheme}://{uri.netloc}\".format(uri=urlparse(url))\n",
    "    liens = {}\n",
    "    for balise_a in soupe.find_all(\"a\", href=True):\n",
    "        lien = urljoin(url, balise_a[\"href\"])\n",
    "        if domaine in lien:\n",
    "            liens[lien] = url  # Stocker la source du lien\n",
    "    \n",
    "    return {\"titre\": titre, \"url\": url, \"premier_paragraphe\": premier_paragraphe, \"liens\": liens}\n",
    "\n",
    "print(\"Extraction du contenu pr√™te √† √™tre utilis√©e !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logique de crawling impl√©ment√©e avec stockage des r√©sultats sp√©cifique √† chaque URL de d√©part !\n"
     ]
    }
   ],
   "source": [
    "# Parties 3 et 4 : Logique de crawling et stockage des URL crawl√©es\n",
    "\n",
    "class Crawler:\n",
    "    def __init__(self, url_depart, max_pages=50): #Arr√™t √† 50 pages\n",
    "        self.url_depart = url_depart\n",
    "        self.max_pages = max_pages\n",
    "        self.pages_visitees = set()\n",
    "        self.pages_a_visiter = [url_depart]\n",
    "        self.resultats = []\n",
    "\n",
    "    def explorer(self):\n",
    "        \"\"\"Parcourt les pages en suivant les liens, priorisant ceux contenant 'product'.\"\"\"\n",
    "        while self.pages_a_visiter and len(self.pages_visitees) < self.max_pages:\n",
    "            url = self.pages_a_visiter.pop(0)\n",
    "            if url in self.pages_visitees or not autorise_crawl(url):  # Ne pas pop les URL n'autorisant pas le crawl, ni celles d√©j√† visit√©es\n",
    "                continue\n",
    "            print(f\"üîç Exploration de: {url}\")\n",
    "            html = telecharger_page(url)\n",
    "            if not html:                            # Ne pas parser les pages non √©crites en HTML\n",
    "                continue\n",
    "            contenu = parser_html(html, url)\n",
    "            self.resultats.append(contenu)\n",
    "            self.pages_visitees.add(url)\n",
    "            \n",
    "            # Ajout des nouveaux liens en priorisant ceux qui contiennent 'product'\n",
    "            nouveaux_liens = sorted(contenu[\"liens\"].keys(), key=lambda x: \"product\" not in x)\n",
    "            self.pages_a_visiter.extend(nouveaux_liens)\n",
    "            \n",
    "            attendre()\n",
    "        self.sauvegarder_resultats()\n",
    "    \n",
    "    \n",
    "    # Ici, pour crawler √† partir de plusieurs URL de d√©part diff√©rentes, on stocke les sorties JSON sous des noms diff√©rents, en utilisant la biblioth√®que \"re\"\n",
    "    def sauvegarder_resultats(self):\n",
    "        \"\"\"Sauvegarde des r√©sultats dans un fichier JSON unique pour chaque URL de d√©part.\"\"\"\n",
    "        identifiant_unique = re.sub(r'[^a-zA-Z0-9]', '_', self.url_depart)[:50]  # Nettoyage et limitation de la longueur\n",
    "        nom_fichier = f\"resultats_{identifiant_unique}.json\"\n",
    "        with open(nom_fichier, \"w\", encoding=\"utf-8\") as fichier:\n",
    "            json.dump(self.resultats, fichier, indent=4, ensure_ascii=False)\n",
    "        print(f\"Exploration termin√©e ! R√©sultats sauvegard√©s dans '{nom_fichier}'.\")\n",
    "\n",
    "print(\"Logique de crawling impl√©ment√©e avec stockage des r√©sultats sp√©cifique √† chaque URL de d√©part !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Exploration de: https://web-scraping.dev/products\n",
      "üîç Exploration de: https://web-scraping.dev/products?category=apparel\n",
      "üîç Exploration de: https://web-scraping.dev/products?category=consumables\n",
      "üîç Exploration de: https://web-scraping.dev/products?category=household\n",
      "üîç Exploration de: https://web-scraping.dev/product/1\n",
      "üîç Exploration de: https://web-scraping.dev/product/2\n",
      "üîç Exploration de: https://web-scraping.dev/product/3\n",
      "üîç Exploration de: https://web-scraping.dev/product/4\n",
      "üîç Exploration de: https://web-scraping.dev/product/5\n",
      "üîç Exploration de: https://web-scraping.dev/products?page=1\n",
      "Exploration termin√©e ! R√©sultats sauvegard√©s dans 'resultats_https___web_scraping_dev_products.json'.\n",
      "üîç Exploration de: https://ensai.fr\n",
      "üîç Exploration de: https://ensai.fr/\n",
      "üîç Exploration de: https://ensai.fr/en/\n",
      "üîç Exploration de: https://ensai.fr/actu-et-evenements/\n",
      "üîç Exploration de: https://ensai.fr/contactez-nous/\n",
      "üîç Exploration de: https://ensai.fr/1-ensai/grande-ecole-data-science/\n",
      "üîç Exploration de: https://ensai.fr/1-ensai/engagements-et-valeurs/\n",
      "üîç Exploration de: https://ensai.fr/egalite-et-diversite/\n",
      "üîç Exploration de: https://ensai.fr/developpement-durable-et-responsabilite-societale/\n",
      "üîç Exploration de: https://ensai.fr/1-ensai/labels-accreditations/\n",
      "Exploration termin√©e ! R√©sultats sauvegard√©s dans 'resultats_https___ensai_fr.json'.\n",
      "üîç Exploration de: https://insee.fr\n",
      "üîç Exploration de: https://insee.fr#contenu\n",
      "üîç Exploration de: https://insee.fr/fr/information/2411108\n",
      "üîç Exploration de: https://insee.fr/fr/information/5894637\n",
      "üîç Exploration de: https://insee.fr/langue/en\n",
      "üîç Exploration de: https://insee.fr/fr/information/2008400\n",
      "üîç Exploration de: https://insee.fr#menu\n",
      "üîç Exploration de: https://insee.fr/fr/accueil\n",
      "üîç Exploration de: https://insee.fr/fr/statistiques\n",
      "üîç Exploration de: https://insee.fr/fr/statistiques?idfacette=1\n",
      "Exploration termin√©e ! R√©sultats sauvegard√©s dans 'resultats_https___insee_fr.json'.\n",
      "üîç Exploration de: https://le-recensement-et-moi.fr\n",
      "üîç Exploration de: https://le-recensement-et-moi.fr#main\n",
      "üîç Exploration de: https://le-recensement-et-moi.fr/\n",
      "üîç Exploration de: https://le-recensement-et-moi.fr/cest-utile\n",
      "üîç Exploration de: https://le-recensement-et-moi.fr/cest-simple\n",
      "üîç Exploration de: https://le-recensement-et-moi.fr/cest-sur\n",
      "üîç Exploration de: https://le-recensement-et-moi.fr/reponses\n",
      "üîç Exploration de: https://le-recensement-et-moi.fr/repondre-en-ligne-au-recensement-de-la-population\n",
      "üîç Exploration de: https://le-recensement-et-moi.fr/cest-utile/\n",
      "üîç Exploration de: https://le-recensement-et-moi.fr/cest-simple/\n",
      "Exploration termin√©e ! R√©sultats sauvegard√©s dans 'resultats_https___le_recensement_et_moi_fr.json'.\n"
     ]
    }
   ],
   "source": [
    "# Partie 5 : Test du crawler sur diff√©rentes pages (Donnant chacune une sortie JSON sp√©cifique)\n",
    "\n",
    "def tester_crawler(url_depart, max_pages):\n",
    "    \"\"\"Test du crawler sur diff√©rentes pages de d√©part et gestion des erreurs courantes.\"\"\"\n",
    "    try:\n",
    "        crawler = Crawler(url_depart, max_pages)\n",
    "        crawler.explorer()\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du test du crawler : {e}\")\n",
    "        \n",
    "# Exemple d'utilisation\n",
    "\n",
    "# Nous arr√™tons √† 10 liens par URL de d√©part pour r√©duire le temps d'exploration\n",
    "tester_crawler(\"https://web-scraping.dev/products\", 10)\n",
    "tester_crawler(\"https://ensai.fr\", 10)\n",
    "tester_crawler(\"https://insee.fr\", 10)\n",
    "tester_crawler(\"https://le-recensement-et-moi.fr\", 10)\n",
    "\n",
    "# Dans VScode, actualiser le contenu du fichier Web_Crawler (√† gauche) pour voir apparaitre les sorties JSON\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
